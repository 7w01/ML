# 决策树(Decision Tree)

## 输入： 

训练集 D={($x_1$,$y_1$), ($x_2$,$y_2$),...,($x_m$,$y_m$)};	属性集 A={$a_1$,$a_2$,$a_3$,...,$a_d$};



## 过程：

***TreeGenerate(D, A):***

​	**if** D中样本全属于同一类别C:

​		将node标记为C类叶结点;

​		return;

​	**if** A = $\varnothing$ or D 中样本在A上取值相同:

​		将node标记位叶结点，其类别标记为D中样本数最多的类;

​		return;

​	**else if:**

​		**从A中选择最优分划属性$a_*$;**

​		for $a_*$的每一个值 $a_*^v$ do:

​			为node生成一个分支，令$D_v$表示D中在$a_*$上取值为$a_*^v$的样本子集；

​			if $D_v$=$\varnothing$:

​				将分支结点标记为叶结点，其类别标记为D中样本最多的类;

​				return;

​			else:

​				以**TreeGenerate($D_v$, A\\{$a_*$})**为分支结点;



**算法关键在于选择怎样的策略去选择划分属性，不同算法基于不同的选择方式**



## 指标：



#### 信息熵：

C为类别总数，$p_k$表示在样本集合D中第k类样本所占的比例为$p_k$，则D的信息熵定义为：
$$
Ent(D)=-\sum^{C}_{k=1}p_klog_2p_k
$$
假定离散属性a有V个可能的取值，若使用a来对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了在属性a上取值为$a^v$的样本，记为$D^v$. 我们可以算出$D^v$的信息熵*Ent*($D^v$)，再考虑不同分支结点的样本数不同，样本数多的分支影响大，故引出信息增益：



#### 信息增益：

$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{D_v}{D}Ent(D_v)
$$

一般而言，**信息增益越大**，意味着使用属性a进行划分所获得的**纯度更大**. 这种方法可能对取值数目较多的属性有所偏好，故不直接使用信息增益，而是使用增益率来选择属性：



#### 增益率：

$$
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$

其中
$$
IV(a)=-\sum^{V}_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$
称为a的**固有值**. 属性a的可能取值数目V越大，则*IV*(a)的值通常会越大



#### 基尼指数：

**基尼值：**
$$
Gini(D)=\sum^{C}_{k=1}\sum_{k'\neq k}p_kp_{k'}\\
=1-\sum^{C}_{k=1}p^2_k
$$
直观上，*Gini(D)*反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率. 因此*Gini(D)*越小，数据集D的纯度越高.

**基尼系数：**
$$
Gini\_index(D,a)=\sum^{V}_{v=1}\frac{|D^v|}{|D|}Gini(D^v)
$$
于是在选择分划后基尼指数最小的属性.







